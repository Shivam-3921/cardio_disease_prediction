{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_curve, RocCurveDisplay, PrecisionRecallDisplay, auc, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    " The dataset is loaded and unnecessary columns, such as `id`, are removed and `age` is converted into yeears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cardio_train_data/cardio_train.csv\",sep=\";\")\n",
    "df.drop([\"id\"],axis=1,inplace=True)     #dropping the \"id\" column\n",
    "df[\"age\"] = df[\"age\"]/365   #converting age into years\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the dataset for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the features into numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique values each feature has\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"gender\",\"cholesterol\",\"gluc\",\"smoke\",\"alco\",\"active\",\"cardio\"]\n",
    "numerical_columns = [\"age\",\"height\",\"weight\",\"ap_hi\",\"ap_lo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Pre-processing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating boxplot for `ap_lo` and `ap_hi` and filtering unrealistic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "sns.boxplot(df, x= \"ap_lo\", ax = ax[0])\n",
    "sns.boxplot(df, x= \"ap_hi\", ax = ax[1])\n",
    "ax[0].set_title(\"ap_lo\")\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_title(\"ap_hi\")\n",
    "ax[1].set_xlabel(None)\n",
    "fig.suptitle(\"Box Plot of `ap_hi` and `ap_lo`\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The blood pressure values should fall within a physiologically realistic range. Values outside this range will be filtered. We choose the thershold of 370/360 mm Hg as given [here](<https://pubmed.ncbi.nlm.nih.gov/7741618/#:~:text=The%20highest%20pressure%20recorded%20in,005).&text=BP%20was%20recorded%20in%2010,maximal%20lifting%20with%20slow%20exhalation.>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filetering unrealistic blood pressure values\n",
    "df = df[(df[\"ap_hi\"] <= 370) & (df[\"ap_hi\"] >= 0)]\n",
    "df = df[(df[\"ap_lo\"] <= 360) & (df[\"ap_lo\"] >= 0)].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "sns.boxplot(df, x= \"ap_lo\", ax = ax[0])\n",
    "sns.boxplot(df, x= \"ap_hi\", ax = ax[1])\n",
    "ax[0].set_title(\"ap_lo\")\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_title(\"ap_hi\")\n",
    "ax[1].set_xlabel(None)\n",
    "fig.suptitle(\"Box Plot of `ap_hi` and `ap_lo`\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating histogram plot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(numerical_columns)/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(numerical_columns):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.histplot(data = df, x = col, bins= 20, hue = \"cardio\", ax=ax[r,c],kde= True)\n",
    "    ax[r,c].set_ylabel(None)\n",
    "k += 1\n",
    "r = int(k//3)\n",
    "c = int(k%3)\n",
    "fig.delaxes(ax=ax[r,c])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating boxplot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cardio\"] = df[\"cardio\"].astype(str)\n",
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(numerical_columns)/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(numerical_columns):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.boxplot(data = df, x = col, y = \"cardio\", ax=ax[r,c])\n",
    "k += 1\n",
    "r = int(k//3)\n",
    "c = int(k%3)\n",
    "fig.delaxes(ax=ax[r,c])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical features is scaled using standard scaler as the data is normally distributed which is evident from the histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit(df[numerical_columns])\n",
    "scaled_numerical_data = scaler.transform(df[numerical_columns])\n",
    "df[numerical_columns] = scaled_numerical_data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correlation between numerical features using correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numerical_columns].corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, no two numerical features are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing categorical features ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countplot for each categorical features\n",
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(categorical_columns)/3)), ncols=3, figsize = (12,12))\n",
    "for k,col in enumerate(categorical_columns[:-1]):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.countplot(data = df, x = col, ax=ax[r,c])\n",
    "    ax[r,c].set_ylabel(None)\n",
    "sns.countplot(data=df, x = \"cardio\", ax=ax[2,1])\n",
    "fig.delaxes(ax=ax[2,0])\n",
    "fig.delaxes(ax=ax[2,2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating countplot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cardio\"] = df[\"cardio\"].astype(str)\n",
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(categorical_columns[:-1])/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(categorical_columns[:-1]):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.countplot(data = df, x = col, hue = \"cardio\", ax=ax[r,c])\n",
    "    ax[r,c].set_ylabel(None)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking relation between categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates chi^2 and p-value from the contigency table between two features\n",
    "def calc_chi2(df, feature1, feature2):\n",
    "    contingency_table = pd.crosstab(df[feature1],df[feature2],margins= True)\n",
    "    chi2_value, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    return chi2_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_matrix = np.zeros((len(categorical_columns), len(categorical_columns)))    #matrix for storing chi^2 values between all the features\n",
    "p_matrix = np.zeros_like(chi2_matrix)   #matrix for storing p-values between all the features\n",
    "\n",
    "for i,col1 in enumerate(categorical_columns):\n",
    "    for j,col2 in enumerate(categorical_columns):\n",
    "        chi2_value, p_value = calc_chi2(df,col1,col2)\n",
    "        chi2_matrix[i,j] = chi2_value\n",
    "        p_matrix[i,j] = p_value\n",
    "\n",
    "chi2_matrix = pd.DataFrame(data=chi2_matrix, index=categorical_columns, columns=categorical_columns) \n",
    "p_matrix = pd.DataFrame(data=p_matrix, index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "sns.heatmap(chi2_matrix, ax = ax[0], cmap=\"Reds\")\n",
    "sns.heatmap(p_matrix, ax=ax[1], cmap=\"Blues_r\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p_value of the feature `alco` as compared with `cardio` is greater than 0.05, which implies that `alco` is independent of the target variable `cardio`. Also, The p_value of the feature `alco` as compared with other categorical features is less than 0.05, which implies that `alco` is dependent on all other categorical features. Thus, we can safely discard `alco` from out ddataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"alco\"], axis = 1, inplace = True) #dropping the \"alco\" feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into trainning and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"cardio\"],axis=1)\n",
    "y = df[\"cardio\"].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (10,5))\n",
    "sns.countplot(data=df, x = y_train, ax = ax[0])\n",
    "ax[0].set_title(\"Training\")\n",
    "sns.countplot(data=df, x = y_test, ax = ax[1])\n",
    "ax[1].set_title(\"Test\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C = 0.5)\n",
    "LR = LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(LR, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(LR, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training the K Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 500)\n",
    "KNN = KNN.fit(X_train,y_train)\n",
    "y_pred = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(KNN, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(KNN, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier with Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training Kernel Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel = \"rbf\", gamma= \"auto\", C= 1.0)\n",
    "SVM = SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(SVM, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(SVM, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree = DecisionTreeClassifier(criterion= \"gini\", max_depth = 5)\n",
    "Decision_Tree = Decision_Tree.fit(X_train, y_train)\n",
    "y_pred = Decision_Tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Decision_Tree, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(Decision_Tree, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC = BaggingClassifier(n_estimators=50, max_samples= 0.75)\n",
    "BC = BC.fit(X_train,y_train)\n",
    "y_pred = BC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(BC, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(BC, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest = RandomForestClassifier(n_estimators=50, criterion= \"entropy\")\n",
    "Random_Forest = Random_Forest.fit(X_train,y_train)\n",
    "y_pred = Random_Forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Random_Forest, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(Random_Forest, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient_Boost = GradientBoostingClassifier(learning_rate= 0.05, n_estimators= 100)\n",
    "Gradient_Boost  = Gradient_Boost.fit(X_train, y_train)\n",
    "y_pred = Gradient_Boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Gradient_Boost, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(Gradient_Boost, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and training AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier(n_estimators=100)\n",
    "AdaBoost = AdaBoost.fit(X_train, y_train)\n",
    "y_pred = AdaBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy scores using confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "\n",
    "print(f\"{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ROC_AUC_Curve and Precision_Recall_Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(AdaBoost, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= roc_auc).from_estimator(AdaBoost, X_test, y_test, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have calculated the accuracy scores for each model without performing any cross validation or paramater tuning. The best model so far is Support Vector Machine with the accurcacy score 73.48% and F1 score 71.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LR\" : {\n",
    "        \"model\": LogisticRegression(penalty=\"l2\"),\n",
    "        \"params_grid\": {\n",
    "            \"C\" : [0.1,1,10,100]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"KNN\" : {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params_grid\": {\n",
    "            \"n_neighbors\" : [10,100,200,500]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(gamma=\"auto\"),\n",
    "        \"params_grid\": {\n",
    "            \"kernel\" : [\"linear\", \"rbf\"],\n",
    "            \"C\": [0.1,1,10,100]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Decision_Tree\" : {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params_grid\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [3,5,10]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Bagging\":{\n",
    "        \"model\": BaggingClassifier(),\n",
    "        \"params_grid\": {\n",
    "            \"n_estimators\" : [10, 50, 100, 500],\n",
    "            \"max_samples\": [0.5, 0.75, 1]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"Random_Forest\": {\n",
    "        \"model\" : RandomForestClassifier(),\n",
    "        \"params_grid\" : {\n",
    "            \"n_estimators\": [10,50,100,500],\n",
    "            \"criterion\" : [\"gini\", \"entropy\"]\n",
    "        }\n",
    "\n",
    "    },\n",
    "\n",
    "    \"Gradient_Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params_grid\": {\n",
    "            \"learning_rate\" : [0.01, 0.05, 0.1, 0.5],\n",
    "            \"n_estimators\": [50,100,500,1000]\n",
    "        }\n",
    "\n",
    "    },\n",
    "\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(),\n",
    "        \"params_grid\": {\n",
    "            \"n_estimators\": [10,50,100,500],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for model_name, param in models.items():\n",
    "    classifier = GridSearchCV(estimator= param[\"model\"], param_grid= param[\"params\"], cv= 10)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        \"model\" : model_name,\n",
    "        \"best_params\" : classifier.best_params_,\n",
    "        \"best_score\" : classifier.best_score_\n",
    "    })\n",
    "\n",
    "scores = pd.DataFrame(scores, columns= [\"model\", \"best_params\", \"best_score\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter grids are created for each of the above classifier and {the best classification model with parameters} is selected as our final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
