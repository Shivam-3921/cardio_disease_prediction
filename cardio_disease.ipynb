{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, RocCurveDisplay, PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    " The dataset is loaded and unnecessary columns, such as `id`, are removed and `age` is converted into years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cardio_data/cardio.csv\",sep=\";\")\n",
    "df.drop([\"id\"],axis=1,inplace=True)     #dropping the \"id\" column\n",
    "df[\"age\"] = df[\"age\"]/365   #converting age into years\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the dataset for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the features into numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique values each feature has\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"gender\",\"cholesterol\",\"gluc\",\"smoke\",\"alco\",\"active\",\"cardio\"]\n",
    "numerical_columns = [\"age\",\"height\",\"weight\",\"ap_hi\",\"ap_lo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Pre-processing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating boxplot for `ap_lo` and `ap_hi` and filtering unrealistic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "sns.boxplot(df, x= \"ap_lo\", ax = ax[0])\n",
    "sns.boxplot(df, x= \"ap_hi\", ax = ax[1])\n",
    "ax[0].set_title(\"ap_lo\")\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_title(\"ap_hi\")\n",
    "ax[1].set_xlabel(None)\n",
    "fig.suptitle(\"Box Plot of 'ap_hi' and 'ap_lo'\")     #Boxplot of `ap_hi` and `ap_lo`\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The blood pressure values should fall within a physiologically realistic range. Values outside this range will be filtered. We choose the thershold of 370/360 mm Hg as given [here](<https://pubmed.ncbi.nlm.nih.gov/7741618/#:~:text=The%20highest%20pressure%20recorded%20in,005).&text=BP%20was%20recorded%20in%2010,maximal%20lifting%20with%20slow%20exhalation.>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filetering unrealistic blood pressure values\n",
    "df = df[(df[\"ap_hi\"] <= 370) & (df[\"ap_hi\"] >= 0)]\n",
    "df = df[(df[\"ap_lo\"] <= 360) & (df[\"ap_lo\"] >= 0)].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "sns.boxplot(df, x= \"ap_lo\", ax = ax[0])\n",
    "sns.boxplot(df, x= \"ap_hi\", ax = ax[1])\n",
    "ax[0].set_title(\"ap_lo\")\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_title(\"ap_hi\")\n",
    "ax[1].set_xlabel(None)\n",
    "fig.suptitle(\"Box Plot of 'ap_hi' and 'ap_lo'\")     #Boxplot of `ap_hi` and `ap_lo`\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating histogram plot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(numerical_columns)/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(numerical_columns):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.histplot(data = df, x = col, bins= 20, hue = \"cardio\", ax=ax[r,c],kde= True)\n",
    "    ax[r,c].set_ylabel(None)\n",
    "k += 1\n",
    "r = int(k//3)\n",
    "c = int(k%3)\n",
    "fig.delaxes(ax=ax[r,c])\n",
    "plt.suptitle(\"Histogram plots for numerical features\")      #Histogram plot for numerical features\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating boxplot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cardio\"] = df[\"cardio\"].astype(str)\n",
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(numerical_columns)/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(numerical_columns):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.boxplot(data = df, x = col, y = \"cardio\", ax=ax[r,c])\n",
    "k += 1\n",
    "r = int(k//3)\n",
    "c = int(k%3)\n",
    "fig.delaxes(ax=ax[r,c])\n",
    "plt.suptitle(\"Boxplots for numerical features\")     #Boxplot for numerical features\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical features is scaled using standard scaler as the data is normally distributed which is evident from the histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit(df[numerical_columns])\n",
    "scaled_numerical_data = scaler.transform(df[numerical_columns])\n",
    "df[numerical_columns] = scaled_numerical_data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correlation between numerical features using correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numerical_columns].corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data= corr_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, no two numerical features are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing categorical features ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(categorical_columns)/3)), ncols=3, figsize = (12,12))\n",
    "for k,col in enumerate(categorical_columns[:-1]):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.countplot(data = df, x = col, ax=ax[r,c])\n",
    "    ax[r,c].set_ylabel(None)\n",
    "sns.countplot(data=df, x = \"cardio\", ax=ax[2,1])\n",
    "fig.delaxes(ax=ax[2,0])\n",
    "fig.delaxes(ax=ax[2,2])\n",
    "plt.suptitle(\"Countplots for categorical features\")     #countplot for each categorical features\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating countplot with `cardio` as hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cardio\"] = df[\"cardio\"].astype(str)\n",
    "fig, ax = plt.subplots(nrows= int(np.ceil(len(categorical_columns[:-1])/3)), ncols=3, figsize = (12,6))\n",
    "for k,col in enumerate(categorical_columns[:-1]):\n",
    "    r = int(k//3)\n",
    "    c = int(k%3)\n",
    "    sns.countplot(data = df, x = col, hue = \"cardio\", ax=ax[r,c])\n",
    "    ax[r,c].set_ylabel(None)\n",
    "plt.suptitle(\"Countplots with 'cardio' as hue\")     #countplot with `cardio` as hue\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking relation between categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates chi^2 and p-value from the contigency table between two features\n",
    "def calc_chi2(df, feature1, feature2):\n",
    "    contingency_table = pd.crosstab(df[feature1],df[feature2],margins= True)\n",
    "    chi2_value, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    return chi2_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_matrix = np.zeros((len(categorical_columns), len(categorical_columns)))    #matrix for storing chi^2 values between all the features\n",
    "p_matrix = np.zeros_like(chi2_matrix)   #matrix for storing p-values between all the features\n",
    "\n",
    "#using calc_chi2 to calculate chi^2-values and p-values between each categorical variables\n",
    "for i,col1 in enumerate(categorical_columns):\n",
    "    for j,col2 in enumerate(categorical_columns):\n",
    "        chi2_value, p_value = calc_chi2(df,col1,col2)\n",
    "        chi2_matrix[i,j] = chi2_value\n",
    "        p_matrix[i,j] = p_value\n",
    "\n",
    "chi2_matrix = pd.DataFrame(data=chi2_matrix, index=categorical_columns, columns=categorical_columns) \n",
    "p_matrix = pd.DataFrame(data=p_matrix, index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "#heatmap for chi^2-values and p-values\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "sns.heatmap(chi2_matrix, ax = ax[0], cmap=\"Reds\")\n",
    "sns.heatmap(p_matrix, ax=ax[1], cmap=\"Blues_r\")\n",
    "ax[0].set_aspect(\"equal\")\n",
    "ax[0].set_title(\"Chi^2 values\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "ax[1].set_title(\"p values\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p_value of the feature `alco` as compared with `cardio` is greater than 0.05, which implies that `alco` is independent of the target variable `cardio`. Also, The p_value of the feature `alco` as compared with other categorical features is less than 0.05, which implies that `alco` is dependent on all other categorical features. Thus, we can safely discard `alco` from out ddataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"alco\"], axis = 1, inplace = True)     #dropping the \"alco\" feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into trainning and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"cardio\"],axis=1)\n",
    "y = df[\"cardio\"].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the counts for training and test dataset\n",
    "fig, ax = plt.subplots(1,2,figsize = (10,5))\n",
    "sns.countplot(data=df, x = y_train, ax = ax[0])\n",
    "ax[0].set_title(\"Training\")\n",
    "sns.countplot(data=df, x = y_test, ax = ax[1])\n",
    "ax[1].set_title(\"Test\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test have similar distribution of target variable `cardio`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\" : [0.001, 0.01, 0.1,1,10], \"solver\": [\"lbfgs\",\"newton-cholesky\", \"saga\"]}\n",
    "classifier = GridSearchCV(estimator= LogisticRegression(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter grids are created for various classification models and the best three model with the best parameters are selected for further prediction. Here, recall score is selected because we want to increase the chance of predicting True Positive compared to False Negative.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Recall} = \\frac{\\text{True Postive}}{\\text{True Postive} + \\text{False Negative}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C = best_params[\"C\"], solver= best_params[\"solver\"]).fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(LR, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(LR, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_neighbors\" : [10,50,100,200,500]}\n",
    "classifier = GridSearchCV(estimator= KNeighborsClassifier(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors= best_params[\"n_neighbors\"]).fit(X_train, y_train)\n",
    "y_pred = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(KNN, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(KNN, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\" : [1e-6,1e-5,1e-4,1e-3], \"loss\": [\"hinge\", \"squared_hinge\"]}\n",
    "classifier = GridSearchCV(estimator= LinearSVC(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_SVM = LinearSVC(C = best_params[\"C\"], loss= best_params[\"loss\"]).fit(X_train, y_train)\n",
    "y_pred = Linear_SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Linear_SVM, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(Linear_SVM, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\" : [0.01,0.1,1], \"gamma\": [\"scale\",1,10,100]}\n",
    "classifier = GridSearchCV(estimator= SVC(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(C = best_params[\"C\"], gamma = best_params[\"gamma\"]).fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(SVM, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(SVM, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"criterion\": [\"gini\", \"entropy\"],\"max_depth\": [5,10,20]}\n",
    "classifier = GridSearchCV(estimator= DecisionTreeClassifier() , param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Tree = DecisionTreeClassifier(criterion = best_params[\"criterion\"], max_depth=best_params[\"max_depth\"]).fit(X_train, y_train)\n",
    "y_pred = Decision_Tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Decision_Tree, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(Decision_Tree, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\" : [50, 100,500,1000], \"max_samples\": [0.5, 0.75, 1]}\n",
    "classifier = GridSearchCV(estimator= BaggingClassifier(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC = BaggingClassifier(n_estimators=best_params[\"n_estimators\"], max_samples= best_params[\"max_samples\"])\n",
    "BC = BC.fit(X_train,y_train)\n",
    "y_pred = BC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(BC, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(BC, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [100,500,1000], \"criterion\" : [\"gini\", \"entropy\"]}\n",
    "classifier = GridSearchCV(estimator= RandomForestClassifier(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest = RandomForestClassifier(n_estimators = best_params[\"n_estimators\"], criterion= best_params[\"criterion\"])\n",
    "Random_Forest = Random_Forest.fit(X_train,y_train)\n",
    "y_pred = Random_Forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Random_Forest, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(Random_Forest, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "#### Defining parameter grid and performing Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"learning_rate\" : [0.1, 0.5, 1, 10],\"n_estimators\": [3,5,10,50]}\n",
    "classifier = GridSearchCV(estimator= GradientBoostingClassifier(), param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and predicting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient_Boost = GradientBoostingClassifier(learning_rate= best_params[\"learning_rate\"], n_estimators= best_params[\"n_estimators\"])\n",
    "Gradient_Boost  = Gradient_Boost.fit(X_train, y_train)\n",
    "y_pred = Gradient_Boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(Gradient_Boost, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(Gradient_Boost, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting\n",
    "#### Defining parameter grid and performing cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [10,50,100]}\n",
    "classifier = GridSearchCV(estimator= AdaBoostClassifier, param_grid= param_grid,cv= ShuffleSplit(n_splits=5, test_size=0.3), scoring= \"recall\", n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "parameters = pd.DataFrame(classifier.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by = 'rank_test_score')\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = classifier.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifier and predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier(n_estimators= best_params[\"n_estimators\"]).fit(X_train, y_train)\n",
    "y_pred = AdaBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix and accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "con_matrix = pd.DataFrame(data = con_matrix, columns= [\"Predicted 0\",\"Predicted 1\"], index= [\"Actual 0\", \"Actual 1\"])\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred) *100\n",
    "recall = recall_score(y_test, y_pred) *100\n",
    "f1 = f1_score(y_test,y_pred) *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1_score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting ROC curve and Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,6))\n",
    "PrecisionRecallDisplay(precision= precision, recall= recall).from_estimator(AdaBoost, X_test, y_test, ax= ax[0])\n",
    "RocCurveDisplay(fpr = fpr,tpr = tpr, roc_auc= auc(fpr,tpr)).from_estimator(AdaBoost, X_test, y_test, ax= ax[1])\n",
    "ax[0].set_title(\"Precision-Recall Curve\")\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
